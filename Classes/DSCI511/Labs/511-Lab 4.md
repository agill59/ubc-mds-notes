```python
# Initialize Otter
import otter
grader = otter.Notebook("lab4.ipynb")
```

# Lab 4: Functions, Classes and Testing

## Instructions
Follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/).


```python
# #Run this cell
import pandas as pd
import warnings
warnings.filterwarnings('ignore') #pandas can get pretty verbose!
```

Code Quality
rubric={quality:5}

The code that you write for this assignment will be given one overall grade for code quality, see our code quality rubric as a guide to what we are looking for. Also, for this course (and other MDS courses that use Python), we are trying to follow the PEP 8 code style. There is a guide you can refer to: https://peps.python.org/pep-0008/

Each code question will also be assessed for code accuracy (i.e., does it do what it is supposed to do?).

### INTRODUCTION

In this lab, you'll be looking at a dataset called MASSIVE which contains ~1M sentences across 50 languages. It is a "parallel corpus", meaning that the same sentences appear in all languages. Two sentences with the same ID in different languages can be considered 'translations' of each other. (Strictly speaking they are not, they are each translations of the English sentence with that ID, but we can ignore that nuance for today.) 

Sentences are referred to as "utterances" in the context of MASSIVE, and they represent things that people would say to virtual assistants, such as "set an alarm", "remind me to water my plants", "what time is my next appointment", etc. Utterances are available as plain text, or with semantic annotation.

You can examine the full dataset on HuggingFace here: https://huggingface.co/datasets/AmazonScience/massive

To keep things manageable for the lab, we are using a slim version of MASSIVE consisting of just 5 languages with 600 utterances each: English, French, German, Korean, and Vietnamese. The data is available in 5 files named `"{language}_massive_data.csv"` placed in the `data` subdirectory. For example you can load the English data like this:


```python
#Run this cell
english = pd.read_csv('data/English_massive_data.csv', encoding='utf-8')
english.head(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>language</th>
      <th>split</th>
      <th>id</th>
      <th>utt</th>
      <th>annot_utt</th>
      <th>scenario</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>English</td>
      <td>test</td>
      <td>0</td>
      <td>wake me up at five am this week</td>
      <td>wake me up at [time : five am] [date : this week]</td>
      <td>alarm</td>
    </tr>
    <tr>
      <th>1</th>
      <td>English</td>
      <td>test</td>
      <td>3</td>
      <td>quiet</td>
      <td>quiet</td>
      <td>audio</td>
    </tr>
    <tr>
      <th>2</th>
      <td>English</td>
      <td>test</td>
      <td>8</td>
      <td>pink is all we need</td>
      <td>[color_type : pink] is all we need</td>
      <td>iot</td>
    </tr>
    <tr>
      <th>3</th>
      <td>English</td>
      <td>test</td>
      <td>14</td>
      <td>and the darkness has fallen</td>
      <td>and the darkness has fallen</td>
      <td>iot</td>
    </tr>
    <tr>
      <th>4</th>
      <td>English</td>
      <td>test</td>
      <td>19</td>
      <td>olly turn the lights off in the bedroom</td>
      <td>olly turn the lights off in the [house_place :...</td>
      <td>iot</td>
    </tr>
  </tbody>
</table>
</div>



Each language file has the same 7 columns:
- `language` is the name of a language
- `split` Each language is split into 'train', 'test', and 'validation' sets. An utterance will only appear in one of these sets.
- `id` is an utterance id. Within a language, each utterance has a unique id. Across languages, utterances will have the same id if they are 'translations' of each other.
- `utt` Raw text of an utterance, written in the conventional orthography of a language
- `annot_utt` An annotated version of the utterance, where some words may have semantic labels indicated by square brackets
- `scenario` The general topic of the sentence (news, alarms, music, datetime, etc.)

## Exercise 1.1: get_massive_data()
rubric={autograde:12}

### Description

Your first task is to write a new function that takes list of language names, and returns a single pandas dataframe with all the relevant data from the CSV files. Someone would be able to call your function like this:

`massive = get_massive_data(languages=['English', 'French'], split='test')`

Don't forget to write a docstring, explaining what your function does!

### Signature

`get_massive_data(languages: list, split_type: str) -> pandas.DataFrame`

### Arguments

`languages` is a list of languages. This argument is required and should not have a default value. 

`split_type` Options are 'test', 'train', 'validation', or 'all'. This argument is optional, and the default value should be 'all'.

### Return value
The function should return a pandas DataFrame. Use the utterance id as your index. Note that this will create non-unique indexes, since the same id values are used across languages.  Be sure to convert the id to an integer! 

The DataFrame should have these columns:

<table>
<tr><td>'language'</td>	    <td>the name of a language (passed as part of the `languages` argument in this function)</td></tr>
<tr><td>'text'</td>	 	    <td>a natural language sentence (corresponds to the 'utt' column in MASSIVE)</td></tr>
<tr><td>'annotation'</td> 	<td>the same sentence with semantic labelling on words (corresponds to the 'annot_utt' column in MASSIVE)</td></tr>
<tr><td>'scenario'</td>	    <td>a semantic label for a scenario (corresponds to 'scenario' column in MASSIVE)</td></tr>
</table>

For example, with English, French and German loaded, index 0 looks like this.
<table>
<tr><th>id</th>	<th>language</th>	<th>text</th>	<th>annotation</th>	<th>scenario</th></tr>
			
<tr><td>0</td>	<td>en-US</td>	<td>wake me up at five am this week</td>	<td>wake me up at [time : five am] [date : this week]</td>	<td>alarm</td></tr>
<tr><td>0</td>	<td>fr-FR</td>	<td>rÃ©veille-moi Ã  cinq heures du matin cette semaine</td>	<td>rÃ©veille-moi Ã  [time : cinq heures du matin]</td> <td>alarm</td></tr>
<tr><td>0</td>	<td>de-DE</td>	<td>wecke mich in dieser woche um fÃ¼nf uhr auf</td>	<td>wecke mich in [date : dieser woche] um [time :...</td>	<td>alarm</td></tr>
</table>

### Hint
Write a loop to load each dataset individually and then use panda's concatenate function



```python
def get_massive_data(languages, **kwargs):
    """
    Takes a list of language names and optionally a split,  and outputs a single dataframe with the relevant data

    Parameters
    ----------
    languages : [str]
                the list of languages
    kwargs    : split : str #unsure how to format kwargs in NumPy/SciPy style
                the type of data to extract : {test, validation, train}

    Returns
    -------
    DataFrame
        the dataframe that contains all relevant data

    Examples
    --------
    >>> get_massive_data(["English"])
    600 x 4 Dataframe of all english data
    """

    massive = pd.DataFrame()
    if (not kwargs):
        for language in languages:
            single_data = pd.read_csv(
                f'data/{language}_massive_data.csv', encoding='utf-8')
            single_data.index = single_data['id']
            clean_single_data = single_data[['language', 'utt', 'annot_utt', 'scenario']].rename(
                {"utt": "text", "annot_utt": "annotation"}, axis=1)
            massive = pd.concat([massive, clean_single_data])

        return massive
    elif (kwargs['split'] in ["train", "test", "validation"]):

        for language in languages:
            single_data = pd.read_csv(
                f'data/{language}_massive_data.csv', encoding='utf-8')
            single_data = single_data[single_data['split'] == kwargs['split']]
            single_data.index = single_data['id']
            clean_single_data = single_data[['language', 'utt', 'annot_utt', 'scenario']].rename(
                {"utt": "text", "annot_utt": "annotation"}, axis=1)
            massive = pd.concat([massive, clean_single_data])

        return massive
```


```python
grader.check("q1_1")
```




<p><strong><pre style='display: inline;'>q1_1</pre></strong> passed! ðŸŒŸ</p>



<!-- BEGIN QUESTION -->

## Exercise 1.2: error handling for `get_massive_data()`
rubric={accuracy: 5}

Create a new function `get_massive_data_2()` by modifying `get_massive_data()` to handle incorrect language inputs, like 'zz-BB' or 'Navajo'. If a language name is not recognized, then the function should "fail gracefully". This means that instead of raising an error and stopping, it should skip over the incorrect name and keep processing the rest of the list. Before returning, your function should print a warning that contains all the languages that didn't work correctly. You can simply print this warning to screen with `print()`, you do not need to raise an actual Python Warning. 


```python
def get_massive_data_2(languages, **kwargs):
    massive = pd.DataFrame()
    bad_langs = []
    if (not kwargs):
        for language in languages:
            if (language in ["English", "French", "German", "Korean", "Vietnamese"]):
                single_data = pd.read_csv(
                    f'data/{language}_massive_data.csv', encoding='utf-8')
                single_data.index = single_data['id']
                clean_single_data = single_data[['language', 'utt', 'annot_utt', 'scenario']].rename(
                    {"utt": "text", "annot_utt": "annotation"}, axis=1)
                massive = pd.concat([massive, clean_single_data])
            else:
                bad_langs.append(language)
        if len(bad_langs) == 0:
            return massive
        else:
            print(f"These languages did not work correct: {bad_langs}")
            return massive
    elif (kwargs['split'] in ["train", "test", "validation"]):

        for language in languages:
            if (language in ["English", "French", "German", "Korean", "Vietnamese"]):
                single_data = pd.read_csv(
                    f'data/{language}_massive_data.csv', encoding='utf-8')
                single_data = single_data[single_data['split']
                                          == kwargs['split']]
                single_data.index = single_data['id']
                clean_single_data = single_data[['language', 'utt', 'annot_utt', 'scenario']].rename(
                    {"utt": "text", "annot_utt": "annotation"}, axis=1)
                massive = pd.concat([massive, clean_single_data])
            else:
                bad_langs.append(language)
        if len(bad_langs) == 0:
            return massive
        else:
            print(f"These languages did not work correct: {bad_langs}")
            return massive
```


```python
#HELPER CELL
#If you were not able to complete Exercise 1, then you can use the following function as a replacement for the rest of the lab. 
#Just uncomment the code and run the cell.
#It returns a correctly formatted DataFrame object that you can use. 
#Note that it only returns 3 languages: English, French, and German. This is enough to pass all subsequent exercises.

# import pickle
# def get_massive_data(languages, split='all'):
#     with open('data/massive_dataframe.pkl', mode='rb') as f:
#         massive = pickle.load(f)
#     return massive
```

<!-- END QUESTION -->

## Exercise 2.1: get_translations()
rubric={autograde:12}

### Description
Now that you have MASSIVE formatted as a dataframe, your next task is to write a search function that takes an utterance id, and returns all the translations of that utterance. For example, to get the translations for utterance 17 we could do this:

```
languages = ['English', 'Korean']
massive = get_massive_data(languages)
utterance_17 = get_translations(massive, 17)
```

Don't forget to add a docstring to this function, explaining what it does.

### Signature

`get_translations(massive: pd.DataFrame, utterance_id: int, annotations: bool) -> pandas.DataFrame`

### Arguments

`massive` is a DataFrame containing MASSIVE data. Ideally this should be the output of your `get_massive_data` function. However, if you were unable to pass all of the tests, you can run the "helper cell" below which get a properly formatted DataFrame for you.

`utterance_id` is an integer representing an utterance id

`annotations` is a boolean. If True, the function return values from the 'annotations' column, if False then it return values from the 'text' column. The default is False.

### Return value
This returns a DataFrame where rows are indexed by language, and there is one column called either 'text' or 'annotation' (depending on the argument supplied to the function).



```python
def get_translations(data, uid, **kwargs):
    """Extracts translations of a given utternace id, returns annotations if true, default is false and returns text"""
    if (not kwargs):
        filtered_data = data[data.index == uid]
        filtered_data.index = filtered_data.language
        return (filtered_data[['text']])
    elif (kwargs['annotations'] == True):
        filtered_data = data[data.index == uid]
        filtered_data.index = filtered_data.language
        return (filtered_data[['annotation']])
```


```python
grader.check("q2_1")
```




<p><strong><pre style='display: inline;'>q2_1</pre></strong> passed! ðŸŒŸ</p>



## Exercise 2.2: error handling for get_translations()
rubric = {accuracy: 5}

<!-- BEGIN QUESTION -->

Create a new function `get_translations_2` by modifying `get_translations()` to handle invalid utterance IDs. If an invalid ID is passed, then the function should return an empty dictionary.


```python
def get_translations_2(data, uid, **kwargs):
    if (not kwargs or kwargs['annotations'] == False):
        filtered_data = data[data.index == uid]
        if (filtered_data.empty):
            return {}
        else:
            filtered_data.index = filtered_data.language
            return (filtered_data[['text']])
    elif (kwargs['annotations'] == True):
        filtered_data = data[data.index == uid]
        if (filtered_data.empty):
            return {}
        else:
            filtered_data.index = filtered_data.language
            return (filtered_data[['annotation']])
```

<!-- END QUESTION -->

## Exercise 3: get_slot_translations()
rubric={autograder:12}

### Description
The utterances in MASSIVE have two kinds of semantic labels: a "scenario", which is the general topic of the whole utterance, and "slots", which are specific words or phrases. Scenarios have their own column in the data. Slots have to be extracted from the text in the "annotations" column, where are they are indicated by square brackets.

For example, utterance 7 in the 'audio' scenario has this annotation:

`pause for [time : ten seconds]	`

This means the slot named 'time' has a value of 'ten seconds'. The slot names are always in English. Utterance 7 in fr-FR is annotated like this:

`pause pendant [time : dix secondes]`

For this exercise, you'll write a function that takes a scenario as input, and returns a table of translations for each slot in each utterance in that scenario. Extracting the slot values requires using regular expressions, which we didn't cover during class, so the code for creating a `slot_name` and `slot_value` column is included in the solution for you. 

Someone would be able to use your function like this:

```
massive = get_massive_data(['English', 'French'])
audio_slots = get_slot_translations(massive, 'audio')
```

Don't forget to add a docstring to your function, explaining what it does.

### Signature
`get_slot_translations(massive: pd.DataFrame, scenario: str) -> pd.DataFrame`


### Arguments
`massive` a pandas dataframe with MASSIVE data. Required, no default value.

`scenario` a string representing one of the scenarios in MASSIVE. Required, no default value.

### Return value
A dataframe indexed by utterance id. The first column is called 'slot_name', and the remaining columns show translations of that slot for each language passed into the function. Some utterances in MASSIVE don't have any slots. Fill any missing values with the string 'no slots'. 

For example, if this function were called with the English, French and German languages in the 'takeaway' scenario, the head of the output table would look like this:

<table>
    <tr><th>id</th><th>slot_name</th><th>English</th><th>French</th><th>German</th></tr>
    <tr><td>51</td><td>order_type</td><td>delivery</td><td>livraison</td><td>lieferoptionen</td></tr>
    <tr><td>52</td><td>time</td><td>delivery</td><td>livraison</td><td>liefer</td></tr>
    <tr><td>53</td><td>time</td><td>delivery</td><td>livraison</td><td>liefer</td></tr>
    <tr><td>54</td><td>time</td><td>delivery</td><td>livraison</td><td>liefer</td></tr>
    <tr><td>55</td><td>food_type</td><td>curry</td><td>piment</td><td>curry</td></tr>
</table>

### Hint
There is already code provided for you to add the `slot_value` and `slot_name` columns to your table. You need to reshape the data into the correct format.


```python
def get_slot_translations(massive, scenario_name):
    """Filters massive dataframe by scenario name, then extracts slot names in each language"""
    massive = massive[massive['scenario'] == scenario_name]
    # ---Don't delete these line! This code finds the necessary slots for you and adds them to the dataframe
    pattern = r'\[([^:\[\]]+) : ([^\[\]]+)\]'
    massive[['slot_name', 'slot_value']
            ] = massive['annotation'].str.extract(pattern)
    # ---
    massive = massive.pivot(columns='language')
    massive = massive[['slot_name', 'slot_value']].reset_index()
    massive['slot_name'] = massive['slot_name'][massive['slot_value'].columns[0]]
    for column in massive['slot_value'].columns:
        massive[column] = massive['slot_value'][column]
    massive = massive.droplevel(1, axis=1).drop('slot_value', axis=1)
    massive = massive.transpose().drop_duplicates().transpose()
    massive.index = massive['id']
    massive = massive.set_index('slot_name', append=True)
    massive = massive.fillna("no slots")
    return massive
```


```python
grader.check("q3")
```




<p><strong><pre style='display: inline;'>q3</pre></strong> passed! ðŸš€</p>



## Exercise 4 - Massive class
rubric={autograder:10}

One thing that's a little awkward about our functions so far is that we have to "repeat" the massive variable and pass it around

```
massive = get_massive_data(['Korean', 'German'], split='test')
utt_123 = get_translations(massive, 123)
weather_slots = get_slot_translations(massive, 'weather')
```

For this last exercise, you'll define a Massive object, which contains all these functions, and which can remember the languages. The code above would run this like this instead:

```
massive = Massive(['Korean', 'German'], split='test')
utt_123 = massive.get_translations(123)
weather_slots = massive.get_slot_translation('weather')
```

Don't worry if you didn't pass all the tests in earlier exercises. Your grade in this section depends on your ablity to organize code into a class, and you won't be double-graded on the output of any previous function.

Don't forget to add a doctstring to your class, explaining what it does, and listing out methods and attributes.

### Instance attributes

 - `.languages` A list of strings representing language names

 - `.split` One of 'test', 'train', 'validation', 'all'

 - `.data`  A pandas dataframe

**Note**: The `.languages` and `.split` attributes should be correctly implemented, but the autograder will not test correctness for the `.data` attribute (it only checks that the attribute is defined).

### Class attributes

 - `.all_languages` A list of strings representing all languages for which we have data

### Methods

 - `get_translations()` Takes an utterance id and 'annotations' boolean as input (as in Exercise 2).

 - `get_slot_translations()` Take a scenario name as input (as in Exercise 3)

You are **not graded on the output** of these methods, so don't worry if you had trouble with the previous exercises. The tests for this section will only try to call these functions to check if they exist and accept the appropriate input arguments. There is no test of the return value. You can actually return any truthy value that you want, and you don't actually have to write any real code inside these methods. Do not return a falsey value or `None`, as that will fail autotesting.

### Magic methods

 - `len(massive)` should return the number of languages that were loaded

 - Two instances of this class are equal if they have the same set of languages and the same data split (test, train, validation, all)


```python
class Massive:
    all_languages = []

    def __init__(self, languages, split):
        self.languages = languages
        self.split = split
        self.data = get_massive_data_2(self.languages, split=self.split)
        Massive.all_languages = len(self.languages)

    def get_translations(self, id, **kwargs):
        # if not kwargs:
        #     return get_translations_2(self.data, id)
        # else:
        #     return get_translations_2(self.data, id, annotations=kwargs)
        return True

    def get_slot_translations(self, scenario_name):
        # return get_slot_translations(self.data, scenario_name)
        return True

    def __len__(self):
        return self.all_languages

    def __eq__(self, other):
        if not isinstance(other, Massive):
            return False
        return (not (False in (self.data.language.unique() == other.data.language.unique())) and self.split == other.split and self.data.shape == other.data.shape)
```


```python
grader.check("q4")
```




<p><strong><pre style='display: inline;'>q4</pre></strong> passed! ðŸ™Œ</p>



## Exercise 5 - Test Driven Development
rubric={accuracy:10, reasoning:10}

<!-- BEGIN QUESTION -->

### Description
For the last exercise, you'll take a 'test-driven development' (TDD) approach to designing a function. This exercise does not involve the MASSIVE dataset from earlier.

You will design a function called `count_words()` which takes text as input, and return a dictionary containing the counts of every word in the text.

### Testing
You must write 5 unit tests for this function using `assert` statements. For each test, provide a short comment, one or two sentences, explaining the purpose of the test. In the spirit of TDD, you should start by writing these tests before coding up your function.

### Signature

`count_words(text: str, punctuation: iterable, ignore_case: bool) -> dict`

### Arguments

- `text`, string. The text to be analyzed. Required, no default value.
- `punctuation`, iterable. This can be a string, or list of strings, containing punctuation symbols to remove from to the text. If a falsey value is passed, then all puncutation is retained. Required, no default value.
- `ignore_case`, boolean. If `True`, all text is converted to lowercase. Default is `False`.


### Return value
The function returns a dictionary where keys are words from the text, and values represent how many times the words appeared in the text. Here are some examples of what the output would look like for different argument values (the order of the output dictionary doesn't matter)

`text = 'The large cat chased the small cat.'`

`print(count_words(text, punctuation=[], ignore_case=False))`

`{'The': 1, 'large': 1, 'cat': 1, 'chased': 1, 'the': 1, 'small': 1, 'cat.': 1}`

`print(count_words(text, punctuation='.', ignore_case=False))`

`{'The': 1, 'large': 1, 'cat': 2, 'chased': 1, 'the': 1, 'small': 1}`

`print(count_words(text, punctuation='.', ignore_case=True))`

`{'the': 2, 'large': 1, 'cat': 2, 'chased': 1, 'small': 1}`





```python
def count_words(text, punctuation, **kwargs):
    words = []
    clean_words = []
    punctuation_used = False
    word_count = {}
    if (not (not kwargs)):
        if (kwargs['ignore_case'] == True):
            text = str.lower(text)
    words = str.split(text, " ")
    if (len(punctuation) == 0):
        for word in words:
            if not word_count:
                word_count[word] = 1
            elif word in list(word_count.keys()):
                word_count[word] = word_count[word] + 1
            else:
                word_count[word] = 1
    else:
        for word in words:
            if punctuation in word:
                clean_words.append(
                    list(filter(None, str.split(word, punctuation)))[0])
                punctuation_used = True
            else:
                clean_words.append(word)
        word_count[clean_words[0]] = 0
        if not punctuation_used:
            print("Punctuation was not used")
        for word in clean_words:
            if word in list(word_count.keys()):
                word_count[word] = word_count[word] + 1
            else:
                word_count[word] = 1
    return word_count
```


```python
text = 'The large cat chased the small cat.'

solution_1 = {'The': 1, 'large': 1, 'cat': 1,
              'chased': 1, 'the': 1, 'small': 1, 'cat.': 1}
# Testing for double counts with punctuation and capitalization present
assert count_words(text, punctuation=[], ignore_case=False) == solution_1

# Testing punctuation not used, print to user it is not used
assert count_words(text, punctuation='....', ignore_case=False) == solution_1


solution_2 = {'The': 1, 'large': 1, 'cat': 2,
              'chased': 1, 'the': 1, 'small': 1}
# Testing cat is double counted when punctuation removed
assert count_words(text, punctuation='.', ignore_case=False) == solution_2

solution_3 = {'the': 2, 'large': 1, 'cat': 2, 'chased': 1, 'small': 1}
# Testing the counted twice when all made lower case
assert count_words(text, punctuation='.', ignore_case=True) == solution_3


text_2 = 'The large large Large lArge small cat.'
solution_5 = {'The': 1, 'large': 2, 'Large': 1,
              'lArge': 1, 'small': 1, 'cat.': 1}
# try with ignore_case not provided
assert count_words(text_2, punctuation=[]) == solution_5
```

    Punctuation was not used


<!-- END QUESTION -->

## Exercise 6: (CHALLENGING) - the `os` module
rubric={accuracy:4}

<!-- BEGIN QUESTION -->

Implement a new `massive_from_path()` function that adds a new `path` argument to `get_massive_data()` from Exercise 1. This new argument should represent a string that provides a path to the MASSIVE language data files. Keep in mind that this argument is only a path to a directory. The specific languages to load are still provided by the `languages` argument. You will need to write some code that joins together the path and the language file names.

You cannot solve this problem through regular string joining, because Windows and Mac systems have different ways of creating file paths. Windows uses a '\\' symbol (e.g. `C:\Users\Data\french.csv`) while Mac uses '/' (e.g. `User/Data/french.csv'`). To make your code work across different operating systems, you'll need to look into Python's `os` module. It has a special join method for creating file paths, which can check the operating system and select the correct separator symbol. 

**Note: Do not modify your `get_massive_data()` function from before!**


```python
import os

# def massive_from_path(...):

...
```




    Ellipsis



<!-- END QUESTION -->


